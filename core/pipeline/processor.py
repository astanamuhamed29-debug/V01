"""MessageProcessor â€” thin OODA orchestrator.

Two modes of operation controlled by ``background_mode``:

  **background_mode=True** (production/chat):
    OBSERVE â†’ reply from *existing* graph context â†’ return fast.
    ORIENT + DECIDE run asynchronously via ``asyncio.create_task``.

  **background_mode=False** (tests / CLI / sync):
    Classic sequential: OBSERVE â†’ ORIENT â†’ DECIDE â†’ ACT.

Each stage is a self-contained class with an ``async def run()`` method.
"""

from __future__ import annotations

import asyncio
import logging
import os
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from typing import TYPE_CHECKING

from config import USE_LLM  # noqa: F401 â€” backward compat
from core.context.builder import GraphContextBuilder
from core.context.session_memory import SessionMemory
from core.analytics.drift_monitor import DriftMonitor
from core.graph.api import GraphAPI
from core.graph.model import Edge, Node
from core.insights.engine import InsightEngine
from core.journal.storage import JournalStorage
from core.llm.embedding_service import EmbeddingService
from core.llm_client import LLMClient, MockLLMClient
from core.mood.tracker import MoodTracker
from core.parts.memory import PartsMemory
from core.pipeline.events import EventBus
from core.pipeline.stage_observe import ObserveStage
from core.pipeline.stage_orient import OrientStage
from core.pipeline.stage_decide import DecideStage
from core.pipeline.stage_act import ActStage
from core.pipeline.stage_observe import _sanitize_text  # noqa: F401 â€” backward compat
from core.search.qdrant_storage import QdrantVectorStorage
from core.tools.base import ToolRegistry
from core.tools.memory_tools import build_default_tools

if TYPE_CHECKING:
    from core.analytics.calibrator import ThresholdCalibrator

logger = logging.getLogger(__name__)


@dataclass(slots=True)
class ProcessResult:
    intent: str
    reply_text: str
    nodes: list[Node]
    edges: list[Edge]


class MessageProcessor:
    """Thin orchestrator: OBSERVE â†’ ORIENT â†’ DECIDE â†’ ACT.

    In ``background_mode`` the heavy LLM extraction (ORIENT+DECIDE) is
    deferred to a background ``asyncio.Task`` so that the user receives
    a reply immediately based on the *already accumulated* graph context.
    """

    def __init__(
        self,
        graph_api: GraphAPI,
        journal: JournalStorage,
        qdrant: QdrantVectorStorage,
        session_memory: SessionMemory,
        llm_client: LLMClient | None = None,
        embedding_service: EmbeddingService | None = None,
        calibrator: "ThresholdCalibrator | None" = None,
        use_llm: bool | None = None,  # backward compat, ignored
        event_bus: EventBus | None = None,
        background_mode: bool = False,
    ) -> None:
        self.graph_api = graph_api
        self.journal = journal
        self.session_memory = session_memory
        self.calibrator = calibrator
        self.background_mode = background_mode
        effective_llm = llm_client or MockLLMClient()
        effective_bus = event_bus or EventBus()

        self.context_builder = GraphContextBuilder(graph_api.storage, embedding_service=embedding_service)
        self.pattern_analyzer = self.context_builder.pattern_analyzer
        self.mood_tracker = MoodTracker(graph_api.storage)
        self.parts_memory = PartsMemory(graph_api.storage)

        # Expose for tests / external access
        self.llm_client = effective_llm
        self.event_bus = effective_bus
        self.embedding_service = embedding_service
        self.qdrant = qdrant
        self.live_reply_enabled: bool = os.getenv("LIVE_REPLY_ENABLED", "true").lower() == "true"

        self._calibrator_loaded: set[str] = set()
        self._pending_tasks: list[asyncio.Task] = []

        # Insight engine â€” runs after every analysis pass
        self._insight_engine = InsightEngine(graph_api=graph_api)
        self._drift_monitor = DriftMonitor(graph_api.storage)

        # Tool registry â€” tools are bound per-user in process_message
        self.tool_registry = ToolRegistry()

        # Build OODA stages
        self._observe = ObserveStage(
            journal=journal,
            session_memory=session_memory,
            event_bus=effective_bus,
        )
        self._orient = OrientStage(
            graph_api=graph_api,
            llm_client=effective_llm,
            embedding_service=embedding_service,
            qdrant=qdrant,
            context_builder=self.context_builder,
            calibrator=calibrator,
            session_memory=session_memory,
        )
        self._decide = DecideStage(
            graph_api=graph_api,
            mood_tracker=self.mood_tracker,
            parts_memory=self.parts_memory,
        )
        self._act = ActStage(
            llm_client=effective_llm,
            session_memory=session_memory,
            event_bus=effective_bus,
        )

    # ------------------------------------------------------------------
    # Main entry point
    # ------------------------------------------------------------------

    async def process_message(
        self,
        user_id: str,
        text: str,
        *,
        source: str = "cli",
        timestamp: str | None = None,
    ) -> ProcessResult:
        # Auto-load calibrator thresholds once per user
        if self.calibrator and user_id not in self._calibrator_loaded:
            try:
                await self.calibrator.load(user_id)
            except Exception as exc:
                logger.warning("ThresholdCalibrator.load failed: %s", exc)
            self._calibrator_loaded.add(user_id)

        # 1. OBSERVE â€” sanitise, journal, session, classify
        obs = await self._observe.run(user_id, text, source=source, timestamp=timestamp)

        if self.background_mode:
            return await self._process_background(user_id, obs)

        return await self._process_sync(user_id, obs)

    # ------------------------------------------------------------------
    # Sync path (tests / CLI): OBSERVE â†’ ORIENT â†’ DECIDE â†’ ACT
    # ------------------------------------------------------------------

    async def _process_sync(self, user_id: str, obs) -> ProcessResult:
        """Full sequential pipeline â€” used in tests and sync CLI."""

        # 2. ORIENT â€” LLM extract, graph persist, embed, search, context
        ori = await self._orient.run(user_id, obs.text, obs.intent)

        # 3. DECIDE â€” policy, task links, conflicts, mood
        dec = await self._decide.run(
            user_id=user_id,
            created_nodes=ori.created_nodes,
            created_edges=ori.created_edges,
            retrieved_context=ori.retrieved_context,
            graph_context=ori.graph_context,
        )

        all_edges = [*ori.created_edges, *dec.created_edges]

        # 3b. INSIGHT â€” detect cross-pattern insights from new + historical data
        insight_nodes = await self._insight_engine.run(
            user_id=user_id,
            new_nodes=ori.created_nodes,
            new_edges=all_edges,
            graph_context=ori.graph_context,
        )
        if insight_nodes:
            ori.graph_context["recent_insights"] = [
                {"title": n.name, "description": n.text, "severity": n.metadata.get("severity", "info")}
                for n in insight_nodes
            ]

        # 4. ACT â€” reply, session memory, event
        drift = await self._drift_monitor.evaluate(user_id)
        if drift.alert:
            ori.graph_context["drift_alert"] = drift.to_dict()

        act = await self._act.run(
            user_id=user_id,
            text=obs.text,
            intent=ori.intent,
            created_nodes=ori.created_nodes,
            created_edges=all_edges,
            graph_context=ori.graph_context,
            mood_context=dec.mood_context,
            parts_context=dec.parts_context,
            retrieved_context=ori.retrieved_context,
            policy=dec.policy,
        )

        return ProcessResult(
            intent=ori.intent,
            reply_text=act.reply_text,
            nodes=ori.created_nodes,
            edges=all_edges,
        )

    # ------------------------------------------------------------------
    # Background path (production chat): fast reply â†’ background analysis
    # ------------------------------------------------------------------

    async def _process_background(self, user_id: str, obs) -> ProcessResult:
        """Fast path: reply from existing context, extraction in background.

        1. Build graph context from ALREADY EXTRACTED data (prev messages).
        2. Get current mood snapshot (fast DB read).
        3. Register per-user tools + inject tool descriptions into context.
        4. Generate reply using existing context (1 LLM call).
        5. Spawn background task for: ORIENT + DECIDE + INSIGHT.
        """

        # â”€â”€ Existing context (fast, no LLM) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        graph_context = await self.context_builder.build(user_id)
        mood_context = await self.mood_tracker.get_current(user_id)
        parts_context = graph_context.get("known_parts", [])[:3]

        # â”€â”€ Register per-user tools â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        self._register_user_tools(user_id)
        graph_context["available_tools"] = self.tool_registry.schemas_compact()

        # â”€â”€ Include recent insights in context â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        recent_insights = await self._load_recent_insights(user_id)
        if recent_insights:
            graph_context["recent_insights"] = recent_insights

        # â”€â”€ ACT â€” reply based on accumulated memory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        act = await self._act.run(
            user_id=user_id,
            text=obs.text,
            intent=obs.intent,
            created_nodes=[],
            created_edges=[],
            graph_context=graph_context,
            mood_context=mood_context or {},
            parts_context=parts_context,
            retrieved_context=[],
            policy="REFLECT",
        )

        # â”€â”€ Handle tool calls in reply â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        final_reply = await self._handle_tool_calls(
            user_id=user_id,
            reply_text=act.reply_text,
            text=obs.text,
            intent=obs.intent,
            graph_context=graph_context,
            mood_context=mood_context or {},
            parts_context=parts_context,
        )

        # â”€â”€ Background: heavy LLM extraction + analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        task = asyncio.create_task(
            self._background_analysis(user_id, obs.text, obs.intent),
        )
        self._pending_tasks.append(task)

        return ProcessResult(
            intent=obs.intent,
            reply_text=final_reply,
            nodes=[],
            edges=[],
        )

    async def _background_analysis(
        self, user_id: str, text: str, intent: str,
    ) -> None:
        """ORIENT + DECIDE + INSIGHT in background â€” enriches graph without blocking chat."""
        try:
            ori = await self._orient.run(user_id, text, intent)
            dec = await self._decide.run(
                user_id=user_id,
                created_nodes=ori.created_nodes,
                created_edges=ori.created_edges,
                retrieved_context=ori.retrieved_context,
                graph_context=ori.graph_context,
            )

            all_edges = [*ori.created_edges, *dec.created_edges]

            # Run insight engine on freshly extracted data
            insight_nodes = await self._insight_engine.run(
                user_id=user_id,
                new_nodes=ori.created_nodes,
                new_edges=all_edges,
                graph_context=ori.graph_context,
            )

            drift = await self._drift_monitor.evaluate(user_id)
            if drift.alert:
                logger.warning("Extraction drift alert: %s", drift.to_dict())

            logger.info(
                "BG analysis done: nodes=%d edges=%d policy=%s insights=%d",
                len(ori.created_nodes),
                len(all_edges),
                dec.policy,
                len(insight_nodes),
            )
        except Exception as exc:
            logger.error("Background analysis failed: %s", exc)

    async def flush_pending(self) -> None:
        """Await all background tasks. Call in tests / shutdown."""
        if self._pending_tasks:
            await asyncio.gather(*self._pending_tasks, return_exceptions=True)
            self._pending_tasks.clear()

    # ------------------------------------------------------------------
    # Tool & insight helpers
    # ------------------------------------------------------------------

    def _register_user_tools(self, user_id: str) -> None:
        """Register default tools bound to the current user."""
        if self.tool_registry.tools:
            return  # already registered
        for tool in build_default_tools(
            graph_api=self.graph_api,
            qdrant=self.qdrant,
            user_id=user_id,
            embedding_service=self.embedding_service,
        ):
            self.tool_registry.register(tool)

    async def _load_recent_insights(self, user_id: str, limit: int = 3) -> list[dict]:
        """Load the N most recent INSIGHT nodes from the graph."""
        try:
            insights = await self.graph_api.storage.find_nodes(
                user_id, node_type="INSIGHT", limit=limit,
            )
            insights.sort(
                key=lambda n: n.metadata.get("created_at", n.created_at or ""),
                reverse=True,
            )
            return [
                {
                    "title": n.name or "",
                    "description": (n.text or "")[:200],
                    "severity": n.metadata.get("severity", "info"),
                    "pattern_type": n.metadata.get("pattern_type", ""),
                }
                for n in insights[:limit]
            ]
        except Exception as exc:
            logger.warning("Failed to load insights: %s", exc)
            return []

    async def _handle_tool_calls(
        self,
        user_id: str,
        reply_text: str,
        text: str,
        intent: str,
        graph_context: dict,
        mood_context: dict,
        parts_context: list,
    ) -> str:
        """Parse tool calls in LLM reply, execute, and re-generate if needed."""
        calls = self.tool_registry.parse_tool_calls(reply_text)
        if not calls:
            return reply_text

        # Execute tools
        tool_results: list[str] = []
        for tool_name, args in calls[:3]:  # max 3 tools per turn
            result = await self.tool_registry.dispatch(tool_name, args)
            if result.success:
                import json
                tool_results.append(
                    f"[{tool_name}]: {json.dumps(result.data, ensure_ascii=False, default=str)[:500]}"
                )
            else:
                tool_results.append(f"[{tool_name}]: Ð¾ÑˆÐ¸Ð±ÐºÐ° â€” {result.error}")

        if not tool_results:
            return reply_text

        # Re-generate reply with tool results injected into context
        graph_context["tool_results"] = "\n".join(tool_results)
        act = await self._act.run(
            user_id=user_id,
            text=text,
            intent=intent,
            created_nodes=[],
            created_edges=[],
            graph_context=graph_context,
            mood_context=mood_context,
            parts_context=parts_context,
            retrieved_context=[],
            policy="REFLECT",
        )
        return act.reply_text

    # ------------------------------------------------------------------
    # Aliases & utilities kept on processor for backward compatibility
    # ------------------------------------------------------------------

    async def process(
        self,
        user_id: str,
        text: str,
        *,
        source: str = "cli",
        timestamp: str | None = None,
    ) -> ProcessResult:
        return await self.process_message(user_id=user_id, text=text, source=source, timestamp=timestamp)

    async def build_weekly_report(self, user_id: str) -> str:
        now = datetime.now(timezone.utc)
        week_ago = now - timedelta(days=7)

        snapshots = await self.graph_api.storage.get_mood_snapshots(user_id, limit=30)
        weekly = []
        for snapshot in snapshots:
            ts = snapshot.get("timestamp")
            if not ts:
                continue
            try:
                dt = datetime.fromisoformat(str(ts).replace("Z", "+00:00"))
            except ValueError:
                continue
            if dt >= week_ago:
                weekly.append(snapshot)

        graph_context = await self.context_builder.build(user_id)
        top_parts = graph_context.get("known_parts", [])[:3]
        active_values = graph_context.get("known_values", [])[:5]

        if not weekly:
            part_line = ", ".join(p.get("name") or p.get("key") or "part" for p in top_parts) or "Ð½ÐµÑ‚"
            value_line = ", ".join(v.get("name") or v.get("key") or "value" for v in active_values) or "Ð½ÐµÑ‚"
            return (
                "ðŸ“Š ÐÐµÐ´ÐµÐ»ÑŒÐ½Ñ‹Ð¹ Ð¾Ñ‚Ñ‡Ñ‘Ñ‚\n"
                "Ð—Ð° Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ 7 Ð´Ð½ÐµÐ¹ mood-ÑÑ€ÐµÐ·Ð¾Ð² Ð¿Ð¾ÐºÐ° Ð½ÐµÑ‚.\n"
                f"Ð¢Ð¾Ð¿ Ñ‡Ð°ÑÑ‚ÐµÐ¹: {part_line}\n"
                f"ÐÐºÑ‚Ð¸Ð²Ð½Ñ‹Ðµ Ñ†ÐµÐ½Ð½Ð¾ÑÑ‚Ð¸: {value_line}"
            )

        avg_valence = sum(float(s.get("valence_avg", 0.0)) for s in weekly) / len(weekly)
        avg_arousal = sum(float(s.get("arousal_avg", 0.0)) for s in weekly) / len(weekly)
        avg_dominance = sum(float(s.get("dominance_avg", 0.0)) for s in weekly) / len(weekly)

        labels: dict[str, int] = {}
        for snapshot in weekly:
            label = str(snapshot.get("dominant_label") or "").strip()
            if not label:
                continue
            labels[label] = labels.get(label, 0) + 1
        top_label = max(labels.items(), key=lambda item: item[1])[0] if labels else "Ð½Ðµ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¾"

        part_line = ", ".join(
            f"{p.get('name') or p.get('key') or 'part'} ({p.get('appearances', 1)})"
            for p in top_parts
        ) or "Ð½ÐµÑ‚"
        value_line = ", ".join(v.get("name") or v.get("key") or "value" for v in active_values) or "Ð½ÐµÑ‚"

        return (
            "ðŸ“Š ÐÐµÐ´ÐµÐ»ÑŒÐ½Ñ‹Ð¹ Ð¾Ñ‚Ñ‡Ñ‘Ñ‚\n"
            f"Ð¡Ñ€ÐµÐ·Ð¾Ð²: {len(weekly)}\n"
            f"Ð¡Ñ€ÐµÐ´Ð½ÐµÐµ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ: valence={avg_valence:.2f}, arousal={avg_arousal:.2f}, dominance={avg_dominance:.2f}\n"
            f"Ð§Ð°Ñ‰Ðµ Ð²ÑÐµÐ³Ð¾: {top_label}\n"
            f"Ð¢Ð¾Ð¿ Ñ‡Ð°ÑÑ‚ÐµÐ¹: {part_line}\n"
            f"ÐÐºÑ‚Ð¸Ð²Ð½Ñ‹Ðµ Ñ†ÐµÐ½Ð½Ð¾ÑÑ‚Ð¸: {value_line}"
        )

    # ------------------------------------------------------------------
    # Human feedback loop APIs
    # ------------------------------------------------------------------

    async def submit_emotion_feedback(
        self,
        user_id: str,
        *,
        label: str,
        predicted_confidence: float,
        was_correct: bool,
    ) -> None:
        """Persist explicit user feedback for emotion extraction quality."""
        await self.graph_api.storage.save_signal_feedback(
            user_id=user_id,
            signal_type=f"emotion:{label.strip().lower() or 'unknown'}",
            signal_score=max(0.0, min(1.0, float(predicted_confidence))),
            was_helpful=bool(was_correct),
            sent_at=datetime.now(timezone.utc).isoformat(),
        )

    async def submit_insight_feedback(
        self,
        user_id: str,
        *,
        insight_title: str,
        insight_score: float,
        was_helpful: bool,
    ) -> None:
        """Persist explicit user feedback for generated insights."""
        await self.graph_api.storage.save_signal_feedback(
            user_id=user_id,
            signal_type=f"insight:{insight_title.strip().lower()[:64]}",
            signal_score=max(0.0, min(1.0, float(insight_score))),
            was_helpful=bool(was_helpful),
            sent_at=datetime.now(timezone.utc).isoformat(),
        )
